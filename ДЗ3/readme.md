# Стейкхолдеры
## Список
- Клиенты
- Воркеры
- Менеджеры
- Сотрудники отдела расходников
- Разработчики продукта (*будет позже отобрана*)
- Исследователи MCF
- Топ-менеджмент
- Финотдел
- Подрядчик печенья
  *Включил его, так как здесь не просто интеграция, а партнерские отношения, как я понял (может быть надумал).
  Подрядчик внезапно может сказать, что одному клиенту он больше не будет возить печенье, и так повлияет на требования.*
- Админы
- Юристы

![image](https://github.com/VasiliySoldatkin/AS-course/assets/34882906/68573239-9b1e-4eed-a2c8-e7fba0e0b9c2)

## Объяснения по группировке
**`Топ-менеджмент` в тесной работе, потому что**
- Это их идея создать такую компанию, у них есть стратегическое видение. 
- У них есть определенный ожидания по релизным циклам.
- За ними последнее решение в плане

**`Исследователи` в тесной работе, потому что**
Матчинг и отсев будут постоянно дополняться, развиваться и тд. Исследователи напрямую влияют на Core-поддомен, так как в нем нужно тестировать кучу гипотез => очень часто требования могут меняться. Лучше предсказать большую часть изменений заранее, узнав видение исследователей

**`Клиенты` и `воркеры` в "keep satisfied", потому что**
Они непосредственно пользуются системой, но у них нет беспокойства за нее.
При этом влияние у клиентов/воркеров в разные моменты может быть разное, так как их влияние сильно зависит от спроса/предложения на платформе (*типичный капитализм*).

**`Финотдел` и `Юристы` между keep satisfied и monitor, потому что**
Нужно соблюдать требования CatFinCompliance, вести нормальный учет и соблюдать трудовое право, но кроме этого больше ничего нет + денег много выделили.
Они влияют на некоторые технические нюансы реализации, но не глобально на процесс в MCF (на core subdomain).


**`Менеджеры` и `Разработчики` иногда в тесной работе, а иногда просто информируются, потому что**
они глобально не влияют на стратегию компании, но при этом они могут говорить о текущих проблемах системы (low skill у команды/обман при найме и тд/плохое качество работы). Выполнять роль healthchecker'ова системы.

**`Админов` мониторим и информируем, потому что**
- вся инфраструктуру и так будет передана из happy cat box - нет ограничений
- нет влияния на бизнес
- разработчики тоже будут мониторить систему и фиксить баги - админы саппортят только

**`Отдел расходников` и `Подрядчика печения` мониторим, потому что**
Они (отдел расходников) подстраиваются под нас, вообще с нами почти не связаны (подрядчик), но изменения внутри них иногда могут повлиять на разработку.
Если подрядчик печенья перестанет возить печенье под каждого клиента, придется переделывать систему и интеграцию.
Если в отделе расходников будет работать 1 человек, а заказов будет 100500, работу застопорится.
Так что необходимо иногда думать о них.
# Архитектурный стиль
# Характеристики
- Общая нагрузка на систему не будет превышать 10 заказов в день и 100 клиентов. Воркеров будет около 20 котов - говорит о scalability по заказам/личному кабинету.

**Делегирование:** scalability, availability, agility, testability, deployability, modifiability, evolvability, consistency
- Кол-во матчей = кол-во заказов. Плюс менеджмент говорит про увеличение нагрузки до 10 заказов в минуту. Если придут внешние коты, то будет ещё больше - scalability
- Без назначения воркера на заказ клиент не может получить оперативно услугу - availability
  *Но нужно ли получать её оперативно?*
- Исследователи MCF пока не знаю, что вообще будет в матчинге, поэтому много экспериментируют ([US-060] + общие пожелания) - agility, modifiability, evolvability
- Нужен низкий ТТМ - agility, testability, deployability
- Терять заказы = потерять клиентов - consistency

**Матчинг:** availability, agility, deployability, modifiability, evolvability
- Core subdomain => много будут тестировать гипотез и улучшать старое - evolvability, agility, modifiability
- В [US-300] менеджмент хочет гибко менять шаги - modifiability
- scalability не добавлял, так как нигде не говорилось, что матчинг обязательно должен работать в real-time. Как будто подождать матчинга 2-3 минуты не должно быть проблемой.

**Отбор кандидатов**: modifiability, elasticity, scalability
- В общих пожеланиях было написано про "набор гипотез по отсеву котов" - modifiability
- Возможно, будет наплыв котов и DDOS (пиковая нагрузка) ([US-081]) - elasticity, scalability.

**Менеджмент**: securability, simplicity,
- Менеджеры не хотят делиться ставками со всеми остальными - securability
- Это не Core-поддомен и все сервисы здесь можно делать простыми, чтобы не тратить ресурсы ([US-261])- simplicity

**Исследования по MCF**: simplicity
- Почти ничего про проверку качества не написано в требованиях. Нужна только форма для отправки результатов проверки ([US-180])

**Сборка расходников**: scalability, simplicity
- Кол-во заказов ~ кол-во расходников ~ кол-во клиентов ([US-150]) - scalability
- Сотрудники собирают заказы так, как им комфортно ([US-140]) - simplicity

**Платежка:** testability, debuggability, securability, consistency
- Ошибка с денежными операциями дорого стоит, поэтому нужно все хорошо тестировать - testability
- Если все же произошла ошибка, нужно иметь возможность легко найти причину - debuggability
- `Финотдел` не хочет потерять данные. Это может повлечь неприятные последствия с маски-шоу, поэтому стоит прислушаться - consistency
# Архитектурный стиль
## Микросервисы для всех Core
**Grouped by:** availability, scalability, modifability
**Reasoning:**
- Неограниченный бюджет
- "Команды будут определены после архитектурного решения." - можно сразу распределить их по bounded context'ам
- Нужно тестировать много гипотез (отсев, матчинг)
## Pipeline для матчинга
**Reasoning:**
- В [US-300] менеджмент под map-reduce скорее всего подразумевал то, что решение матчингу основывается на данных и похоже на "воронку": берем всех воркеров и последовательно фильтруем, уходя в все большую и большую детализацию. 
  Так находим самых крутых воркеров под задачу.
- По сути здесь речь идет об обогащении заказа информацией (ETL). На входе заказ без воркера, на выходе заказ с воркером и стоимостью
- Шаги могут часто меняться => лучше сделать монолит, в котором локально можно будет все гибко настраивать и оркестрировать.
- подсчет стоимости идет сразу за матчингом - можно включить его в pipeline обработки.
- Проблемы с масштабируемостью можно игнорировать до некоторой степени, так как нигде не говорилось о том, что матчинг должен был работать в real-time.
# Базы данных
## RDBMS для работы с заказами
- Терять заказы нельзя - consistency
- Структура заказа очень определенная + заказ умеет одну схему на чтения, но при этом разными способами изменяется => schema on write больше подходит для поддержания data integrity
## RDBMS для invoice'ов и всей платежки
**Reasoning:**
- CatFinCompliance скорее всего это требует
## RDBMS для проверки качества, сборки расходников, тотализатора
**Reasoning:**
- RDBMS базы все знают, они простые. Так как это все контексты в supporting 
# Стили коммуникаций
## Sync между наймом и учетом пользователей
**Reasoning:**
После отбора воркера нужно удостовериться, что учетка в основной системе точно заведена. Намного проще sync вызовом сохранить consistency, чем решать проблемы асинхронного взаимодействия.

Учитывая то, что в систему попадают только top 3% воркеров со всей планеты котов и попадают они далеко не сразу (тесты нужно ещё пройти), сервис отбора воркеров не будет сильно нагружать сервисы учета пользователей в моменте (не требуется поддерживать elasticity).
## Async между матчингом и работой с заказами
**Reasoning:**
- В требованиях вроде ничего не говорится про real-time
- Pipeline хуже масштабируется, а нагрузка по заказам будет все больше => нельзя жестко связывать два эти сервиса по масштабируемости.
## Async между работой с заказами и подготовкой расходников
**Reasoning:**
- Подрядчик печения в [US-150] только через 10 минут отправит печения. Раз ждем, значит async
- Не было требований по real-time в требованиях
- Если был бы sync, нужно было бы поддерживать scalability в подготовке расходников (coupling был бы по деплою).
  Так как подготовка расходников - это supporting, стоит его упростить и не думать об автоматическом масштабировании.
## Async между работой с заказами и проверкой качества/тотализатором
**Reasoning:**
- Мы только оповещаем эти Supporting поддомены о том, что что-то произошло с заказами. В целом, можно не думать о consistency (если что-то потеряется или придет не в том порядке - не страшно)

# Фитнес-функции
- Code Coverage на каждый сервис для проверки testability.
  90% coverage
- Линтеры для проверки цикломатической (и остальных видов) сложности - simplicity. Линтер должен выполнятьспроходить
- Нагрузочные тесты с линейной нагрузкой для проверки scalability. Должно поддерживать 10 заказов в минуту.
- Нагрузочные тесты с резкой нагрузкой для проверки elasticity в найме котов. 1к запросов должен сервис поддерживать.
- Story points per sprint для проверки TTM.
- Median Cycle time - для оценки релизного цикла. 
  Релизный цикл для всей системы — месяц, для скоринга работников — неделя максимум.
- Время устранения неизвестной ошибки - для проверки observability (запрос от разрабов и админов). Не дольше часа должны искать ошибку
